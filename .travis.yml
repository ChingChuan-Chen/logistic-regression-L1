language: python
sudo: false
python:
    - 2.7
env:
before_install:
    - wget https://repo.continuum.io/miniconda/Miniconda-latest-Linux-x86_64.sh -O miniconda.sh
    - chmod +x miniconda.sh
    - ./miniconda.sh -b -p /home/travis/mc
    - export PATH=/home/travis/mc/bin:$PATH
install:
    - wget http://d3kbcqa49mib13.cloudfront.net/spark-1.5.2-bin-hadoop2.6.tgz || wget http://www.us.apache.org/dist/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz || wget http://download.nextag.com/apache/spark/spark-1.5.2/spark-1.5.2-bin-hadoop2.6.tgz
    - tar -xvf spark-1.5.2-bin-hadoop2.6.tgz
    - export SPARK_HOME=`pwd`/spark-1.5.2-bin-hadoop2.6
    - conda update --yes conda
    - conda create -n testenv --yes pip nose coverage setuptools numpy scipy scikit-learn python=$TRAVIS_PYTHON_VERSION
    - source activate testenv
    - pip install py4j
    - export PYTHONPATH=$SPARK_HOME/python:$PYTHONPATH
    - ls $SPARK_HOME/python
    - python setup.py install
    - sudo rm -rf /dev/shm && sudo ln -s /run/shm /dev/shm
script:
    - nosetests tests --nologcapture --logging-level=WARN --detailed-errors --verbosity=2 --with-coverage --cover-branches --cover-xml --cover-html --cover-html-dir=./htmlcov --cover-package=logistic_regression_L1 --traverse-namespace
after_success:
    - codecov
notifications:
    email:
        recipients:
            - stzeng@appnexus.com
            - sdevaraju@appnexus.com
